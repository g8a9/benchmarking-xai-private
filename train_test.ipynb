{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "accompanied-communication",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_metric, Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback,\n",
    "    set_seed\n",
    ")\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "hearing-stations",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, class_weights, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.class_weights = class_weights\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\").long()\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        \n",
    "        loss_fct = nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    \n",
    "\n",
    "acc = load_metric(\"accuracy\")\n",
    "f1 = load_metric(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": acc.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
    "        \"f1\": f1.compute(predictions=predictions, references=labels)[\"f1\"],\n",
    "    } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "revolutionary-korea",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 128\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "union-baptist",
   "metadata": {},
   "source": [
    "## EVALITA 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fatty-startup",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7900a01fa1254fba9b6f5ed26738e76c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1693f68990c6439994f74c5635597358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/411 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "550acaab594b4d4cb1ff5581882b2a93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ee516099fa04dc8946db4d6a6a1c48a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/426k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"distilbert-base-cased\" \n",
    "output_dir = model_name + \"_ami18\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def preprocess_text(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "front-olympus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef391de4a989463690772109e4249fe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f25e3af4e94143e08652b426ae1ae60f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29328996345446b388cc44dc24c34726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = pd.read_csv(\"data/miso_train.tsv\", sep=\"\\t\")\n",
    "validation = pd.read_csv(\"data/miso_dev.tsv\", sep=\"\\t\")\n",
    "test = pd.read_csv(\"data/miso_test.tsv\", sep=\"\\t\")\n",
    "\n",
    "raw_datasets = DatasetDict(\n",
    "    train=Dataset.from_pandas(train),\n",
    "    validation=Dataset.from_pandas(validation),\n",
    "    test=Dataset.from_pandas(test)\n",
    ")\n",
    "raw_datasets = raw_datasets.rename_column(\"misogynous\", \"label\")\n",
    "\n",
    "proc_datasets = raw_datasets.map(preprocess_text, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "precious-hartford",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "excess-homework",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=3,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_ratio=0.1,\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    logging_steps=50,\n",
    "    save_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    dataloader_num_workers=4,\n",
    "    report_to=\"wandb\",\n",
    "    metric_for_best_model=\"loss\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "heated-target",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = class_weight.compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(proc_datasets[\"train\"][\"label\"]),\n",
    "    y=np.array(proc_datasets[\"train\"][\"label\"])\n",
    ")\n",
    "class_weights = torch.tensor(class_weights, device=\"cuda\", dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "historical-impression",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStoppingCallback(early_stopping_patience=2)\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    class_weights=class_weights,\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=proc_datasets[\"train\"],\n",
    "    eval_dataset=proc_datasets[\"validation\"],\n",
    "    callbacks=[early_stopping],\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "under-complexity",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text, misogyny_category, target.\n",
      "/home/dauin_user/gattanasio/venvs/unbias_venv/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 3600\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 675\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mg8a9\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/g8a9/huggingface/runs/3ktp0lrb\" target=\"_blank\">roberta-base_ami18</a></strong> to <a href=\"https://wandb.ai/g8a9/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='675' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/675 01:10 < 01:28, 4.25 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.681200</td>\n",
       "      <td>0.519057</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.489200</td>\n",
       "      <td>0.455647</td>\n",
       "      <td>0.792500</td>\n",
       "      <td>0.796069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.503400</td>\n",
       "      <td>0.427674</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.806122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.456800</td>\n",
       "      <td>0.416428</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.796791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.371900</td>\n",
       "      <td>0.454887</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.798851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.309300</td>\n",
       "      <td>0.582734</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>0.721854</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text, misogyny_category, target.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 400\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to roberta-base_ami18/checkpoint-50\n",
      "Configuration saved in roberta-base_ami18/checkpoint-50/config.json\n",
      "Model weights saved in roberta-base_ami18/checkpoint-50/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text, misogyny_category, target.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 400\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to roberta-base_ami18/checkpoint-100\n",
      "Configuration saved in roberta-base_ami18/checkpoint-100/config.json\n",
      "Model weights saved in roberta-base_ami18/checkpoint-100/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text, misogyny_category, target.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 400\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to roberta-base_ami18/checkpoint-150\n",
      "Configuration saved in roberta-base_ami18/checkpoint-150/config.json\n",
      "Model weights saved in roberta-base_ami18/checkpoint-150/pytorch_model.bin\n",
      "Deleting older checkpoint [roberta-base_ami18/checkpoint-50] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text, misogyny_category, target.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 400\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to roberta-base_ami18/checkpoint-200\n",
      "Configuration saved in roberta-base_ami18/checkpoint-200/config.json\n",
      "Model weights saved in roberta-base_ami18/checkpoint-200/pytorch_model.bin\n",
      "Deleting older checkpoint [roberta-base_ami18/checkpoint-100] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text, misogyny_category, target.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 400\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to roberta-base_ami18/checkpoint-250\n",
      "Configuration saved in roberta-base_ami18/checkpoint-250/config.json\n",
      "Model weights saved in roberta-base_ami18/checkpoint-250/pytorch_model.bin\n",
      "Deleting older checkpoint [roberta-base_ami18/checkpoint-150] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text, misogyny_category, target.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 400\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to roberta-base_ami18/checkpoint-300\n",
      "Configuration saved in roberta-base_ami18/checkpoint-300/config.json\n",
      "Model weights saved in roberta-base_ami18/checkpoint-300/pytorch_model.bin\n",
      "Deleting older checkpoint [roberta-base_ami18/checkpoint-250] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from roberta-base_ami18/checkpoint-200 (score: 0.41642773151397705).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=300, training_loss=0.46862095514933266, metrics={'train_runtime': 73.5776, 'train_samples_per_second': 146.784, 'train_steps_per_second': 9.174, 'total_flos': 315733266432000.0, 'train_loss': 0.46862095514933266, 'epoch': 1.33})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "alleged-contamination",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to roberta-base_ami18\n",
      "Configuration saved in roberta-base_ami18/config.json\n",
      "Model weights saved in roberta-base_ami18/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "suited-astrology",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text, misogyny_category, target.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.5903980135917664,\n",
       " 'test_accuracy': 0.668,\n",
       " 'test_f1': 0.6891385767790261,\n",
       " 'test_runtime': 2.9371,\n",
       " 'test_samples_per_second': 340.477,\n",
       " 'test_steps_per_second': 42.56}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = trainer.predict(test_dataset=proc_datasets[\"test\"])\n",
    "predictions.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-fruit",
   "metadata": {},
   "source": [
    "## EVALITA 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "close-optimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"dbmdz/bert-base-italian-cased\"\n",
    "output_dir = model_name + \"_ami20\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def preprocess_text(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "impressed-output",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c52ef361fb4012afd67978da074c9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06f6c89291a84c068695f32e963e7987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b4011010289479fa9eaf6b814bb6b1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = pd.read_csv(\"data/AMI2020_training_raw_90.csv\")\n",
    "validation = pd.read_csv(\"data/AMI2020_validation_raw_10.csv\")\n",
    "test = pd.read_csv(\"data/AMI2020_test_raw_gt.tsv\", sep=\"\\t\")\n",
    "\n",
    "raw_datasets = DatasetDict(\n",
    "    train=Dataset.from_pandas(train),\n",
    "    validation=Dataset.from_pandas(validation),\n",
    "    test=Dataset.from_pandas(test)\n",
    ")\n",
    "raw_datasets = raw_datasets.rename_column(\"misogynous\", \"label\")\n",
    "\n",
    "proc_datasets = raw_datasets.map(preprocess_text, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "current-condition",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-base-italian-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-italian-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "hungarian-hudson",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=3,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_ratio=0.1,\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    logging_steps=50,\n",
    "    save_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    dataloader_num_workers=4,\n",
    "    report_to=\"none\",\n",
    "    metric_for_best_model=\"loss\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "simple-venezuela",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = class_weight.compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(proc_datasets[\"train\"][\"label\"]),\n",
    "    y=np.array(proc_datasets[\"train\"][\"label\"])\n",
    ")\n",
    "class_weights = torch.tensor(class_weights, device=\"cuda\", dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "foster-acceptance",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStoppingCallback(early_stopping_patience=2)\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    class_weights=class_weights,\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=proc_datasets[\"train\"],\n",
    "    eval_dataset=proc_datasets[\"validation\"],\n",
    "    callbacks=[early_stopping],\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cheap-bloom",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: aggressiveness, id, text.\n",
      "/home/dauin_user/gattanasio/venvs/unbias_venv/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 4500\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 843\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='843' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/843 01:06 < 02:00, 4.50 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.651400</td>\n",
       "      <td>0.488335</td>\n",
       "      <td>0.778000</td>\n",
       "      <td>0.792523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.418700</td>\n",
       "      <td>0.388581</td>\n",
       "      <td>0.834000</td>\n",
       "      <td>0.840691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.355100</td>\n",
       "      <td>0.254121</td>\n",
       "      <td>0.884000</td>\n",
       "      <td>0.882114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.337300</td>\n",
       "      <td>0.235381</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.906054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.293100</td>\n",
       "      <td>0.309616</td>\n",
       "      <td>0.888000</td>\n",
       "      <td>0.890625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.244600</td>\n",
       "      <td>0.257937</td>\n",
       "      <td>0.916000</td>\n",
       "      <td>0.912500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: aggressiveness, id, text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to dbmdz/bert-base-italian-cased_ami20/checkpoint-50\n",
      "Configuration saved in dbmdz/bert-base-italian-cased_ami20/checkpoint-50/config.json\n",
      "Model weights saved in dbmdz/bert-base-italian-cased_ami20/checkpoint-50/pytorch_model.bin\n",
      "Deleting older checkpoint [dbmdz/bert-base-italian-cased_ami20/checkpoint-200] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: aggressiveness, id, text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to dbmdz/bert-base-italian-cased_ami20/checkpoint-100\n",
      "Configuration saved in dbmdz/bert-base-italian-cased_ami20/checkpoint-100/config.json\n",
      "Model weights saved in dbmdz/bert-base-italian-cased_ami20/checkpoint-100/pytorch_model.bin\n",
      "Deleting older checkpoint [dbmdz/bert-base-italian-cased_ami20/checkpoint-300] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: aggressiveness, id, text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to dbmdz/bert-base-italian-cased_ami20/checkpoint-150\n",
      "Configuration saved in dbmdz/bert-base-italian-cased_ami20/checkpoint-150/config.json\n",
      "Model weights saved in dbmdz/bert-base-italian-cased_ami20/checkpoint-150/pytorch_model.bin\n",
      "Deleting older checkpoint [dbmdz/bert-base-italian-cased_ami20/checkpoint-50] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: aggressiveness, id, text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to dbmdz/bert-base-italian-cased_ami20/checkpoint-200\n",
      "Configuration saved in dbmdz/bert-base-italian-cased_ami20/checkpoint-200/config.json\n",
      "Model weights saved in dbmdz/bert-base-italian-cased_ami20/checkpoint-200/pytorch_model.bin\n",
      "Deleting older checkpoint [dbmdz/bert-base-italian-cased_ami20/checkpoint-100] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: aggressiveness, id, text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to dbmdz/bert-base-italian-cased_ami20/checkpoint-250\n",
      "Configuration saved in dbmdz/bert-base-italian-cased_ami20/checkpoint-250/config.json\n",
      "Model weights saved in dbmdz/bert-base-italian-cased_ami20/checkpoint-250/pytorch_model.bin\n",
      "Deleting older checkpoint [dbmdz/bert-base-italian-cased_ami20/checkpoint-150] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: aggressiveness, id, text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to dbmdz/bert-base-italian-cased_ami20/checkpoint-300\n",
      "Configuration saved in dbmdz/bert-base-italian-cased_ami20/checkpoint-300/config.json\n",
      "Model weights saved in dbmdz/bert-base-italian-cased_ami20/checkpoint-300/pytorch_model.bin\n",
      "Deleting older checkpoint [dbmdz/bert-base-italian-cased_ami20/checkpoint-250] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from dbmdz/bert-base-italian-cased_ami20/checkpoint-200 (score: 0.23538126051425934).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=300, training_loss=0.3833550294240316, metrics={'train_runtime': 66.8984, 'train_samples_per_second': 201.799, 'train_steps_per_second': 12.601, 'total_flos': 315996377487360.0, 'train_loss': 0.3833550294240316, 'epoch': 1.07})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "effective-wagon",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to dbmdz/bert-base-italian-cased_ami20\n",
      "Configuration saved in dbmdz/bert-base-italian-cased_ami20/config.json\n",
      "Model weights saved in dbmdz/bert-base-italian-cased_ami20/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "living-attack",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: aggressiveness, id, text.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.4890109896659851,\n",
       " 'test_accuracy': 0.769,\n",
       " 'test_f1': 0.7979002624671916,\n",
       " 'test_runtime': 3.103,\n",
       " 'test_samples_per_second': 322.264,\n",
       " 'test_steps_per_second': 40.283}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = trainer.predict(test_dataset=proc_datasets[\"test\"])\n",
    "predictions.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experimental-forestry",
   "metadata": {},
   "source": [
    "## EXPERT ANNOTATIONS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unbias_venv",
   "language": "python",
   "name": "unbias_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
